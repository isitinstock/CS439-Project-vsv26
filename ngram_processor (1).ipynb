{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3200572-fca5-4aa0-8f70-0310f54195c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Reading raw event data...\n",
      "2. Processing events and extracting n-grams for all customers.\n",
      "3. Aggregating 83451 n-gram instances.\n",
      "\n",
      "N-gram processing complete. Results saved to ngram_counts.csv\n",
      "Total unique n-grams found: 584\n",
      "total n-gram instances processed: 83451\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# constants\n",
    "EVENT_WINDOW_DAYS = 30\n",
    "MAX_N_GRAM = 3\n",
    "OUTPUT_PATH = 'ngram_counts.csv'\n",
    "\n",
    "\"\"\" in practice, you would not know what the ngrams would be until you have\n",
    "the simulated data. But since I simulated the data previously, I know what ngrams to\n",
    "look for. In a real situation you would be going off of whatever the customers happened to\n",
    "do. You could extract your own ngrams from the data as you saw fit.\"\"\"\n",
    "\n",
    "def get_ngrams(sequence, max_n):\n",
    "    \"\"\"Generates all n-grams (up to max_n) from a sequence.\"\"\"\n",
    "    ngrams = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        if len(sequence) >= n:\n",
    "            ngrams.append(tuple(sequence[-n:]))\n",
    "    return ngrams\n",
    "\n",
    "def process_customer_events(customer_df):\n",
    "    \"\"\"\n",
    "    Processes a single customer's event history to extract n-grams and\n",
    "    associate them with the 90-day churn target.\n",
    "    \"\"\"\n",
    "    # Sort by date to ensure correct sequence\n",
    "    customer_df = customer_df.sort_values(by='event_date')\n",
    "    # Convert event_date to datetime objects\n",
    "    customer_df['event_date'] = pd.to_datetime(customer_df['event_date'])\n",
    "    # List to store n-gram data for this customer\n",
    "    customer_ngram_data = []\n",
    "    \n",
    "    # Iterate through each event as the \"current\" observation point\n",
    "    for i in range(len(customer_df)):\n",
    "        current_event = customer_df.iloc[i]\n",
    "        current_date = current_event['event_date']\n",
    "        churn_target = current_event['churn_in_90_days']\n",
    "        \n",
    "        # Define the 30-day lookback window\n",
    "        window_start_date = current_date - pd.Timedelta(days=EVENT_WINDOW_DAYS)\n",
    "        \n",
    "        # Filter events within the window (including the current event)\n",
    "        window_events = customer_df[\n",
    "            (customer_df['event_date'] > window_start_date) & \n",
    "            (customer_df['event_date'] <= current_date)\n",
    "        ]\n",
    "        \n",
    "        # Get the sequence of event codes in the window\n",
    "        event_sequence = window_events['event_code'].tolist()\n",
    "        \n",
    "        # Extract n-grams from the end of the sequence (most recent events)\n",
    "        # We only care about the n-grams ending at the current date\n",
    "        ngrams = get_ngrams(event_sequence, MAX_N_GRAM)\n",
    "        \n",
    "        # Record the n-grams and the churn target at this point in time\n",
    "        for ngram in ngrams:\n",
    "            customer_ngram_data.append({\n",
    "                'ngram': ngram,\n",
    "                'count': 1,\n",
    "                'churn_count': churn_target\n",
    "            })\n",
    "            \n",
    "    return customer_ngram_data\n",
    "\n",
    "def aggregate_ngrams(ngram_data):\n",
    "    \"\"\"Aggregates counts and churn counts for all unique n-grams.\"\"\"\n",
    "    ngram_stats = defaultdict(lambda: {'count': 0, 'churn_count': 0})\n",
    "    \n",
    "    for item in ngram_data:\n",
    "        ngram_key = item['ngram']\n",
    "        ngram_stats[ngram_key]['count'] += item['count']\n",
    "        ngram_stats[ngram_key]['churn_count'] += item['churn_count']\n",
    "        \n",
    "    # Convert to a list of dictionaries for DataFrame creation\n",
    "    aggregated_list = []\n",
    "    for ngram, stats in ngram_stats.items():\n",
    "        aggregated_list.append({\n",
    "            'ngram': ' '.join(ngram),\n",
    "            'n_size': len(ngram), # number of events in the sequence\n",
    "            'count': stats['count'],\n",
    "            'churn_count': stats['churn_count']\n",
    "        })\n",
    "        \n",
    "    return pd.DataFrame(aggregated_list)\n",
    "\n",
    "# MAIN\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"1. Reading raw event data...\")\n",
    "    try:\n",
    "        raw_events_df = pd.read_csv('raw_events.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: raw_events.csv not founnd run data_simulator.py first.\")\n",
    "        exit()\n",
    "\n",
    "    # group by customer and process\n",
    "    print(\"2. Processing events and extracting n-grams for all customers.\")\n",
    "    all_ngram_data = []\n",
    "    \n",
    "    # customer_groups = raw_events_df.groupby('customer_id').head(10) \n",
    "    customer_groups = raw_events_df.groupby('customer_id')\n",
    "    \n",
    "    for cust_id, group in customer_groups:\n",
    "        all_ngram_data.extend(process_customer_events(group))\n",
    "        \n",
    "    print(f\"3. Aggregating {len(all_ngram_data)} n-gram instances.\")\n",
    "    ngram_df = aggregate_ngrams(all_ngram_data)\n",
    "    \n",
    "    # Calculate raw churn rate. we will bayesian smooth later\n",
    "    ngram_df['raw_churn_rate'] = ngram_df['churn_count'] / ngram_df['count']\n",
    "    \n",
    "    # Save the aggregated counts\n",
    "    ngram_df.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"\\nN-gram processing complete. Results saved to {OUTPUT_PATH}\")\n",
    "    print(f\"Total unique n-grams found: {len(ngram_df)}\")\n",
    "    print(f\"total n-gram instances processed: {ngram_df['count'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc7fde-30d7-44ef-a592-0d54f3242b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
